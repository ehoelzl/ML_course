{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids, headers = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studying the distribution of variables to detect possible categorical or faulty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_feature_distribution\n",
    "#fig = plot_feature_distribution(tX, headers, np.arange(len(headers)), \"Distribution of features\", 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig.savefig(\"feature_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from these plots, different features have different distributions. We can also notice that there seems to be one categorical feature, namely `PRI_jet_num`. This feature is the number of jets used during the experiment. If we look at the documentation of the dataset (https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf), we can see that some feature are actually affected by the number of jets used.\n",
    "\n",
    "Features affected by undefined values:\n",
    "- `DER_mass_MMC` ID=0: undefined if topology of event too far from expected\n",
    "- `DER_deltaeta_jet_jet` ID=4 : undefined if `PRI_jet_num` <= 1\n",
    "- `DER_mass_jet_jet` ID=5: undefined if `PRI_jet_num` <= 1\n",
    "- `DER_prodeta_jet_jet` ID=6: undefined if `PRI_jet_num` <= 1\n",
    "- `DER_lep_eta_centrality` ID=12: undefined if `PRI_jet_num` <= 1\n",
    "- `PRI_jet_leading_pt` ID=23: undefined if `PRI_jet_num` == 0\n",
    "- `PRI_jet_leading_eta` ID=24: undefined if `PRI_jet_num` == 0\n",
    "- `PRI_jet_leading_phi` ID=25: undefined if `PRI_jet_num` == 0\n",
    "- `PRI_jet_subleading_pt` ID=26: undefined if `PRI_jet_num` <= 1\n",
    "- `PRI_jet_subleading_eta` ID=27: undefined if `PRI_jet_num` <= 1\n",
    "- `PRI_jet_subleading_phi` ID=28: undefined if `PRI_jet_num` <= 1\n",
    "\n",
    "Hence, it might be a good idea to have estimators for each number of jets (0, 1, and more than 1).\n",
    "Also, we can see that features `PRI_tau_phi`, `PRI_lep_phi` and `PRI_met_phi` have an almost uniform distribution. Let us look at the distribution of the features that are undefined at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import plot_undefined_features\n",
    "#plot_undefined_features(tX, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 5 features (let's call the the `PHI` features) that have a uniform distribution. We can discard those, as they do not bring us any additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044814595684975 DER_sum_pt 9 PRI_met_sumet 21\n",
      "0.9656283889164025 DER_sum_pt 9 PRI_jet_all_pt 29\n",
      "0.8844128574100245 PRI_met_sumet 21 PRI_jet_all_pt 29\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(tX.shape[1]):\n",
    "    for j in np.arange(i+1, tX.shape[1]):\n",
    "        t1 = tX[:, i]\n",
    "        t1 = t1[np.where(t1 > -999)]\n",
    "        t2 = tX[:, j]\n",
    "        t2 = t2[np.where(t2 > -999)]\n",
    "        if t1.shape == t2.shape:\n",
    "            corr = np.corrcoef(np.c_[t1, t2].T)[0][1]\n",
    "            if np.abs(corr) > 0.85:\n",
    "                print(corr, headers[i],i, headers[j], j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, column `DER_sum_pt` at index 9 is very correlated with 2 columns. We choose to drop it, along with all `PHI` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [9, 15, 18, 20, 25, 28]\n",
    "to_keep = [x for x in np.arange(tX.shape[1]) if x not in to_delete]\n",
    "headers = headers[to_keep]\n",
    "tX = tX[:, to_keep]\n",
    "tX[tX == -999] = np.nan\n",
    "#plot_feature_distribution(tX, headers, np.arange(tX.shape[1]), \"Feature distribution\", 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will split the dataset into 6 categories:\n",
    "- `tX_0, y_0` : Features and labels for experiments with 0 jets, that have a defined `DER_mass_MMC`\n",
    "- `tX_0_nm,, y_0_nm`:  Features and labels for experiments with 0 jets, that have an undefined `DER_mass_MMC`\n",
    "- `tX_1, y_1` : Features and labels for experiments with 1 jet, that have a defined `DER_mass_MMC`\n",
    "- `tX_1_nm, y_1_nm`: Features and labels for experiments with 1 jet, that have an undefined `DER_mass_MMC`\n",
    "- `tX_2, y_2` : Features and labels for experiments with more than 1 jets\n",
    "- `tX_2_nm, y_2_nm`: Features and labels for experiments with more than 1 jet, that have an undefined `DER_mass_MMC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import split_dataset\n",
    "\n",
    "jet_column = 18\n",
    "tX_0, y_0, tX_1, y_1, tX_2, y_2 = split_dataset(tX, y, jet_col=jet_column) # Split into each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the dataset, we need to select the columns that are meaningful for each category, and add some features using polynomial expansion, exponential, logarithm and so on. For that, we have created a function `enhance_features`. This function adds all the expansions and performs PCA to project the feature matrix on a new basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(x, y, ratio):\n",
    "    \"\"\" Splits the dataset into train/test data\n",
    "    \n",
    "    :param x: Feature matrix\n",
    "    :param y: Labels\n",
    "    :param ratio: Ratio for train\n",
    "    :return: x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    indices = np.random.permutation(x.shape[0])  # Get random permutations of the indices\n",
    "    num_train = int(ratio * x.shape[0])\n",
    "    train_indices, test_indices = indices[:num_train], indices[num_train:]  # Split indices into train and test\n",
    "    \n",
    "    train_x, train_y = x[train_indices], y[train_indices]\n",
    "    test_x, test_y = x[test_indices], y[test_indices]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import prepare_for_training, prepare_for_testing\n",
    "from implementations import reg_logistic_regression, ridge_regression\n",
    "from cross_validation import cross_validate_degrees\n",
    "\n",
    "def train_ridge_model(x, y, train_ratio, lambda_):\n",
    "    x_train, y_train, x_test, y_test = split_train_test(x, y, train_ratio)\n",
    "    x_train, y_train = prepare_for_training(x_train, y_train, logistic=False)\n",
    "    x_test, y_test = prepare_for_testing(x_test, y_test, logistic=False)\n",
    "    \n",
    "    weights, loss = ridge_regression(y_train, x_train, lambda_)\n",
    "\n",
    "    score = compute_accuracy(y_test, x_test, weights)\n",
    "    print(f\"Ridge regression got score of {score}, loss of {loss}\")\n",
    "    return weights\n",
    "\n",
    "def train_logistic_model(x, y, train_ratio, gamma, lambda_, max_iters):\n",
    "    x_train, y_train, x_test, y_test = split_train_test(x, y, train_ratio)\n",
    "    x_train, y_train = prepare_for_training(x_train, y_train)\n",
    "    x_test, y_test = prepare_for_testing(x_test, y_test)\n",
    "\n",
    "    initial_w = np.zeros((x_train.shape[1], 1))\n",
    "    weights, loss = reg_logistic_regression(y_train, x_train, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    score = compute_accuracy(y_test, x_test, weights)\n",
    "    print(f\"Regularized logisitc regression got score of {score}, loss of {loss}\")\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few global variable declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_expansion import expand_features\n",
    "train_ratio = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cross_validate_degrees(tX_0, y_0, np.logspace(-5, 0, 6), 1e-5, np.arange(1, 10), 4, \"Mean accuracy and loss w.r. to polynomial expansion for PRI_jet_num=0\", jet_column, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"cross_val_0_log.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a degree of 3 gives the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validate_reg(tX_0_exp, y_0, selected_cols_0, 1e-5, np.logspace(-4, 0, 5), 4, \"Mean accuracy and Loss w.r. to Regularization for PRI_jet_num=0, DER_MASS_MMC > -999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing polynomial expansion up to degree 1\n",
      "Matrix has now 78 features\n",
      "Gradient Descent(0/999): loss=62328.487623130844\n",
      "Gradient Descent(100/999): loss=33728.87433724222\n",
      "Gradient Descent(200/999): loss=33395.88013009496\n",
      "Gradient Descent(300/999): loss=33241.93995405746\n",
      "Gradient Descent(400/999): loss=33142.619210348545\n",
      "Gradient Descent(500/999): loss=33069.750495989276\n",
      "Gradient Descent(600/999): loss=33012.19657978584\n",
      "Gradient Descent(700/999): loss=32964.57270272934\n",
      "Gradient Descent(800/999): loss=32923.948718661486\n",
      "Gradient Descent(900/999): loss=32888.57752292921\n",
      "Regularized logisitc regression got score of 0.8395716573258607, loss of 32857.33386451057\n"
     ]
    }
   ],
   "source": [
    "degree_0 = 1\n",
    "tX_0_exp = expand_features(tX_0, degree_0, jet_col=jet_column)\n",
    "w_0 = train_logistic_model(tX_0_exp, y_0, train_ratio, 1e-5, 0, 1000)#train_ridge_model(tX_0_exp, y_0, train_ratio, 1e-2)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cross_validate_degrees(tX_1, y_1, np.logspace(-5, 0, 6), 1e-5, np.arange(1, 10), 4, \"Mean accuracy and loss w.r. to polynomial expansion for PRI_jet_num=1\", jet_column, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"cross_val_1_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing polynomial expansion up to degree 1\n",
      "Matrix has now 94 features\n",
      "Gradient Descent(0/1299): loss=48374.04858409802\n",
      "Gradient Descent(100/1299): loss=33261.26677115651\n",
      "Gradient Descent(200/1299): loss=32391.19495588421\n",
      "Gradient Descent(300/1299): loss=32055.827018345677\n",
      "Gradient Descent(400/1299): loss=31904.26183823479\n",
      "Gradient Descent(500/1299): loss=31799.195787530756\n",
      "Gradient Descent(600/1299): loss=31718.68696477509\n",
      "Gradient Descent(700/1299): loss=31654.65191372418\n",
      "Gradient Descent(800/1299): loss=31602.35647751285\n",
      "Gradient Descent(900/1299): loss=31558.800911624283\n",
      "Gradient Descent(1000/1299): loss=31521.970265731594\n",
      "Gradient Descent(1100/1299): loss=31490.447306527632\n",
      "Gradient Descent(1200/1299): loss=31463.19763675362\n",
      "Regularized logisitc regression got score of 0.7905867182462927, loss of 31439.443292550743\n"
     ]
    }
   ],
   "source": [
    "degree_1 = 1\n",
    "tX_1_exp = expand_features(tX_1, degree_1, jet_col=jet_column)\n",
    "w_1 = train_logistic_model(tX_1_exp, y_1, train_ratio, 1e-5, 0, 1300)#train_ridge_model(tX_1_exp, y_1, train_ratio, 1e-3)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = cross_validate_degrees(tX_2, y_2, np.logspace(-5, 0, 6), 1e-5, np.arange(1, 10), 4, \"Mean accuracy and loss w.r. to polynomial expansion for PRI_jet_num=2\", jet_column, logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"cross_val_2_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing polynomial expansion up to degree 1\n",
      "Matrix has now 126 features\n",
      "Gradient Descent(0/999): loss=45254.19312439771\n",
      "Gradient Descent(100/999): loss=29804.21136727084\n",
      "Gradient Descent(200/999): loss=28152.29942212304\n",
      "Gradient Descent(300/999): loss=27782.11255514223\n",
      "Gradient Descent(400/999): loss=27559.525412432777\n",
      "Gradient Descent(500/999): loss=27406.437432113366\n",
      "Gradient Descent(600/999): loss=27294.011954383517\n",
      "Gradient Descent(700/999): loss=27207.544944503818\n",
      "Gradient Descent(800/999): loss=27138.7219482441\n",
      "Gradient Descent(900/999): loss=27082.486932488697\n",
      "Regularized logisitc regression got score of 0.8209510682288077, loss of 27035.58640590495\n"
     ]
    }
   ],
   "source": [
    "degree_2 = 1\n",
    "tX_2_exp = expand_features(tX_2, degree_2, jet_col=jet_column)\n",
    "w_2 = train_logistic_model(tX_2_exp, y_2, train_ratio, 1e-5, 0, 1000)#train_ridge_model(tX_2_exp, y_2, train_ratio, 1e-3)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import prepare_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test, _ = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = tX_test[:, to_keep]\n",
    "tX_test[tX_test == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_testset(x, degrees, w):\n",
    "    x = expand_features(x, degrees, print_=False, jet_col=jet_column)\n",
    "    x, _ = prepare_for_testing(x,None, logistic=True)\n",
    "    y_pred = predict_labels(w, x)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0_test, ids_0, tX_1_test, ids_1, tX_2_test, ids_2= split_dataset(tX_test, ids_test, jet_col=jet_column) # Split into each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_0 = predict_testset(tX_0_test, degree_0, w_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = predict_testset(tX_1_test, degree_1, w_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = predict_testset(tX_2_test, degree_2, w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = np.concatenate([ids_0, ids_1, ids_2])\n",
    "y_pred = np.concatenate([y_pred_0, y_pred_1, y_pred_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape[0] == tX_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'submission.csv' # TODO: fill in desired name of output file for submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../data/solution-with-features.csv')[['Id', 'Prediction']]\n",
    "pred = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pred.merge(labels, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8190177355263112"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c[c['Prediction_x'] == c['Prediction_y']]) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
